---
title: "Comparing Multiple Groups"
---

## Introduction {#sec-intro-anova}

In Chapter @sec-intro-two-means we learned what to do when we have two groups and want to compare their means. However, not every situation only has two groups.

Picture this: You just ran an experiment and had three groups. You want to see if they performed differently, but do not know how. You learned in the last chapter that if we had two groups, we could run a t.test, but now you have one more. This sounds like the perfect opportunity to run an **Analysis of Variance** (ANOVA). An **ANOVA** tests whether the means of three or more groups are significantly different from one another. It does this by comparing the variance between groups to the variance within groups.

::: callout-note
## In this lesson, we will explore how to use **ANOVAs** to answer the question:

*How do studying methods influence memory scores?*
:::

We will create a dataset called `memory`, which includes studentsâ€™ method of studying and their memory scores.

## Learning Objectives {#sec-anova-objectives}

By the end of this chapter, you will be able to:

-   Create a reproducible dataset in R.
-   Conduct one-way and two-way ANOVAs.
-   Interpret ANOVA output (F, p, SS, MS).
-   Run post-hoc comparisons with Tukeyâ€™s HSD.
-   Compare models using AIC.

Although the statistical method used here is called ANOVA, the goal of this chapter is to understand how group comparisons scale beyond two groups.

## Creating Our Data {#sec-creating-data-anova}

In this chapter, we are not going to be loading any previously captured data. Instead, we will be utilizing R to help create our own data. Our data is going to contain two variables:

1.  method: identified their studying methods, either flashcards, rereading the material, or taking practice tests
2.  score: how well they scored on a memory test after studying.

With our focus being reproducibility, how can we manage to make sure that the data we create here is the same data you will create? In R, we do this by using the `set.seed` command. What this does is makes sure the "random" data creation is the same no matter who is creating the data.

Imagine we had a deck of cards. We need to shuffle them before we deal them to our players. `set.seed()` is the equivalent to that deck of cards being shuffled in a specific order, that way, if it is shuffled that way every single time, each player is going to be dealt the same cards.

::: callout-important
## With regards to set.seed()

As long as we put the same number in `set.seed()` then the same data will be created.
:::

Next, we can use the `rnorm()` command to dictate quantity, mean, and standard deviation of each.

```{r}
#| label: creating_our_data
#| message: false
#| warning: false
library(tidyverse)

set.seed(123)  # ensures reproducibility

memory <- tibble(
  method = rep(c("Flashcards", "Rereading", "Testing"), each = 20),
  score = c(
    rnorm(20, mean = 75, sd = 8),  # Flashcards group
    rnorm(20, mean = 70, sd = 9),  # Rereading group
    rnorm(20, mean = 85, sd = 7)   # Testing group
  )
)

glimpse(memory)
```

We have successfully created our first dataset!

## Descriptive Statistics {#sec-anova-descriptive-stats}

Now that we have our data, let us do some digging to get some descriptive statistics regarding central tendency. As done before, we can create a summarized table, or we can utilize the `favstats` command from the library(mosaic)

```{r}
#| label: tbl-summary_table
#| tbl-cap: A summarized table grouped by the method, with average, standard deviation, and the number of rows.
#| message: false
#| warning: false
# Creating a summarizing table
memory_summarized <- memory %>%
  group_by(method) %>%
  summarize(
    mean_score = mean(score),
    sd_score = sd(score),
    n = n()
  )

memory_summarized
```

Instead of writing all of the code manually, we can use the code below.

```{r}
#| label: tbl-favstats
#| tbl-cap: A summarized table using the function favstats() instead of summarize().
#| message: false
#| warning: false
library(mosaic)

favstats(score ~ method, data = memory)
```

Some things we now see:

1.  Testing has the highest minimum and maximum values in comparison to the other two methods
2.  Testing has the highest median
3.  Testing has the highest mean.
4.  Testing has the lowest standard deviation
5.  There are no missing values

These are early indications that testing is the best studying method if we want the highest memory scores.

## Visualizing Relationships {#sec-anova-viz}

When we have categorical and numeric data, there are two visualizations that are extremely relevant when visualizing our data: boxplots and bar charts (see @sec-ggplot-bar for a quick review on bar charts).

```{r}
#| label: fig-plots-anova
#| fig-cap: "Boxplots showing the distribution of memory test scores for each study technique, highlighting differences in central tendency and variability across groups."
ggplot(memory, aes(x = method, y = score, fill = method)) +
  geom_boxplot(alpha = 0.7) +
  labs(
    title = "Memory Test Scores by Study Technique",
    x = "Study Technique",
    y = "Memory Test Score"
  ) +
  theme_minimal()
```

```{r}
#| label: fig-anova-means-plot
#| fig-cap: "Average memory test scores by study technique. While differences in group means are apparent, statistical significance is assessed using a one-way ANOVA rather than visual inspection alone."
memory %>%
  group_by(method) %>%
  summarize(mean_score = mean(score)) %>%
  ggplot(aes(x = method, y = mean_score, fill = method)) +
  geom_col() +
  geom_text(aes(label = round(mean_score, 1)), vjust = -0.5) + # adds the means to bars
  labs(
    title = "Average Memory Score by Study Technique (ANOVA Means)",
    x = "Study Technique",
    y = "Average Score"
  ) +
  theme_minimal()
```

## Running a T.Test {#sec-anova-t-test}

In @sec-t-tests, we learned how to run a t.test to better understand the difference in means between two groups. If we try to run a t.test on our data now...

```{r}
#| label: ttest
# Last chapter we learned how to run a t.test. Let's try it out now.
try(t.test(score ~ method, data = memory))
```

It does not work! The reason being is that while t.test compare means, they are only able to calculate the difference between **two** means. When there are more than two, t.tests fail. We must instead conduct an *ANOVA.*

## One-Way ANOVA {#sec-one-way-anova}

We have some idea, from both our visuals and descriptive statistics that the testing studying technique is better than the other two. However, while we do know that the means are different, we do not know if the difference between the means of the three techniques is statistically significant or not. We can not use t.tests because there are more than two groups. In order to uncover whether the difference between the studying technique means is statistically significant or not, we now conduct an ANOVA.

To do this, we can use the `aov` command which is very similar to `t.test`. The formula used is:

dependent_variable ~ independent_variable, data = your_data

To really bring it to the next level, we can also call the `supernova` command from the supernova package.

```{r}
#| label: tbl-anova
#| tbl-cap: "One-way ANOVA Summary Table: Studying Method and Memory Scores"
# running an anova
anova_model <- aov(score ~ method, data = memory)

summary(anova_model)
```

We can also utilize the `supernova()` function for a cleaner output.

```{r}
#| label: tbl-anova-supernova
#| tbl-cap: "One-way ANOVA Supernova Table: Studying Method and Memory Scores"
library(supernova)
supernova(anova_model)  # clearer ANOVA table
```

Just as the `aov` command should look similar, the output of running the anova should look familiar. Reviewing the data we get:

-   Sum of Squares (SS): the total amount of variation explained by each source
    -   Method SS - the amount of variation explained by the studying method - 2657
    -   Residual SS- the amount of variation explained by everything other than method - 3063
    -   The total is 5720, which means that **about 46% of all variation in scores is due to studying method.**
-   Df: degrees of freedom
    -   There are 2 degrees of freedom (3-1), since there are 3 studying methods
-   Mean square: SS Ã· Df.
    -   provides us with the average variability per degree of freedom.
    -   Mean Method SS = 2657 / 2 = 1328.5
    -   Mean Residuals SS = 3064 / 57 = 53.8
    -   There is more variation explained per degree of freedom by studying method vs everything else.
-   F value: how much larger the variation between groups is compared to the variation within groups
-   24.72: there is more variation between the studying method groups than within each group.
-   pr(\>F): the p-value
    -   It is lower than 0.05, meaning there is a statistically significant difference in memory scores based on studying technique.
    -   Hint: The number of \* by the p-value indicates if it is significant or not. \*\*\* is statistically significant.

With all of that, we can now conclude that there is a statistically significant difference in memory scores due to studying technique. While this tells us that the difference is significant, it does not tell us **what the difference between the techniques is.** Through visualizations and descriptive statistics, we think testing is the best method, but it is important to make sure.

## Post-hoc Tests {#sec-post-hoc-anova}

To add to our ANOVA and descriptive statistics, we can run what is called *Post-hoc Tests.* These are meant to dive deeper into the ANOVA model. While an ANOVA model tells us **if** there is a statistically significant difference, Post-hoc Tests tell us **where** the difference is. The `TukeyHSD` command provides the insight.

```{r}
#| label: tbl-post-hoc
#| tbl-cap: "Tukeyâ€™s Honest Significant Difference (HSD) Post-hoc Comparisons: Pairwise Mean Differences Between Study Techniques"
TukeyHSD(anova_model)
```

We have three different rows, with each studying technique compared to each other. The â€˜diffâ€™ column shows the mean difference between groups (Group 2 âˆ’ Group 1).

-   Positive = first group has a higher mean.
-   Negative = first group has a lower mean.

Looking here, we can see that testing has a higher mean than both flashcards and rereading when compared against the two. We also look at the p adj, which is just the p-value. All three are statistically significant, specifically the two related to testing.

We can visualize this difference in mean comparisons using the `plot` command in base R.

```{r}
#| label: fig-post-hoc-plot
#| fig-cap: "Tukeyâ€™s HSD post-hoc comparison plot showing confidence intervals for pairwise mean differences between study techniques. Each horizontal line represents a comparison between two groups; intervals that do not cross zero indicate statistically significant differences following a significant ANOVA."
plot(TukeyHSD(anova_model))
```

Taking all this into consideration:

1.  Descriptive statistics show that the mean of Testing is higher than the other techniques.
2.  The ANOVA results show that there is a statistically significant difference between the means of the different techniques.
3.  The Post-hoc test results show that testing has higher means than Flashcards and Rereading, and that those differences are statistically significant.

All of this brings us to the conclusion: **The Testing studying technique provides the highest memory exam scores.**

## Adding a Second Factor (Two-Way ANOVA) {#sec-anova-second-factor}

We have done a fantastic job of identifying the best studying technique out of the three in order to get the highest memory exam scores. In life, there are typically more than just two variables. What if there was another variable in this study? For instance, what if each person's caffeine level while studying for the memory exam was also taken into consideration. Maybe caffeine level had an influence on how well they studied, and in turn impacts their memory score. And if that is taken into account, is that a better model for explaining differences in memory scores?

To test this out, let's first create the caffeine column using a similar technique to how we created our data in the beginning.

```{r}
#| label: adding-caffeine
# Let's say we also measured caffeine intake (low vs high)
set.seed(42)
memory2 <- memory %>%
  mutate(
    caffeine = rep(c("Low", "High"), times = 30))
```

Now that weâ€™ve established that study method affects memory, letâ€™s ask a new question: does caffeine level also play a role? This moves us from a one-way ANOVA (one independent variable) to a two-way ANOVA (two independent variables, or factors)

```{r}
#| label: tbl-interaction
#| tbl-cap: "Two-way ANOVA Supernova Table: Studying Method, Memory Scores, and Caffeine"
# To see the interaction between method and caffeine levels, we add a *
anova_2 <- aov(score ~ method * caffeine, data = memory2)

supernova(anova_2)
```

Interestingly, when we go to the p-values, we see that neither caffeine nor the interaction (how caffeine influences method) are statistically significant. This is a huge indication that caffeine does not have any impact on memory scores whatsoever. We can visualize this

```{r}
#| label: fig-interaction-plot
#| fig-cap: "Interaction plot between Study Method and Caffeine Level. The near-parallel lines indicate that caffeine does not significantly alter the effectiveness of different study methods."

ggplot(memory2, aes(x = method, y = score, color = caffeine, group = caffeine)) +
  stat_summary(fun = mean, geom = "point") +
  stat_summary(fun = mean, geom = "line") +
  labs(title = "Interaction of Study Method and Caffeine",
       x = "Study Method",
       y = "Mean Memory Score") +
  theme_minimal()
```

::: callout-tip
## What the p-value is actually telling you:

In the graph above, we see that the lines for both high and low are nearly parallel. This gives you an indication that caffeine does not have a significant impact on study method
:::

The results from the ANOVA are impactful, and since we now have two different ANOVA models, we can compare the models.

## Model Comparison With AIC {#sec-aic-anova}

Using the `aictab` from the AICcmodavg library, we can compare the one-way ANOVA model with the two-way ANOVA model to see what is the overall better model of understanding differences in memory scores.

```{r}
#| label: tbl-model-comparison
#| tbl-cap: "Model Comparison using AIC: Evaluating One-Way vs. Two-Way ANOVA for Memory Performance"
# Compare one-way vs. two-way models
library(AICcmodavg)
model.set <- list(
  aov(score ~ method, data = memory2),
  aov(score ~ method * caffeine, data = memory2))

model.names <- c("One-way", "Two-way")

aictab(model.set, modnames = model.names)
```

We will review model comparisons in more detail in @sec-more-variables. For right now, we can just look at the AICc values. AIC values donâ€™t test significance â€” they measure relative model quality. The general rule of thumb is that the lower the value, the better the model. In this case, the one-way ANOVA is the better model, confirming what the ANOVA results themselves also described.

## Key Takeaways {#sec-anova-takeaways}

-   ANOVA (Analysis of Variance) is used to compare **means across more than two groups**.
-   A **significant F-statistic** indicates that at least one group mean is different.\
-   **Post-hoc tests** (like Tukeyâ€™s HSD) reveal **which** groups differ from each other.\
-   The **Sum of Squares (SS)** separates total variation into between-group and within-group sources.\
-   The **Mean Square (MS)** is the average variation per degree of freedom (SS Ã· df).\
-   The **F value** is a ratio comparing between-group to within-group variability.\
-   **Two-way ANOVA** adds an additional factor (e.g., caffeine level) to test main effects and interactions.
-   **AIC** (Akaike Information Criterion) helps compare models â€” *lower values indicate better fit*.\
-   Always visualize your data before and after running ANOVA to confirm patterns in group means.
-   Reproducibility matters â€” use `set.seed()` whenever you simulate or randomize data.

## Checklist {#sec-anova-checklist}

**When running an ANOVA, have you:**

-   [ ] Verified that your **dependent variable is numeric** and your **independent variable(s) are categorical**?\
-   [ ] Created or imported your dataset reproducibly (used `set.seed()` if simulated)?\
-   [ ] Summarized descriptive statistics for each group?\
-   [ ] Visualized group means and distributions using boxplots or bar charts?\
-   [ ] Fit your model using `aov()` or `supernova()`?\
-   [ ] Interpreted the **F statistic**, **df**, and **p-value**?\
-   [ ] Conducted **post-hoc tests** (e.g., `TukeyHSD`) to identify where differences lie?\
-   [ ] Checked **residuals** or **assumptions** (optional but good practice)?\
-   [ ] Added a **second factor** if relevant (two-way ANOVA)?\
-   [ ] Compared models using **AIC** or another fit statistic?\
-   [ ] Written a clear interpretation that answers your research question?

## Key Functions & Commands {#sec-anova-keyfunctions}

The following functions and commands are introduced or reinforced in this chapter to support analysis of variance (ANOVA), post-hoc testing, and model comparison.

-   `set.seed()` *(base R)*
    -   Ensures reproducibility when generating random data.
-   `rnorm()` *(stats)*
    -   Generates random values from a normal distribution with a specified mean and standard deviation.
-   `aov()` *(stats)*
    -   Fits one-way and two-way analysis of variance (ANOVA) models.
-   `summary()` *(base R)*
    -   Produces summaries of ANOVA model results, including F statistics and p-values.
-   `supernova()` *(supernova)*
    -   Displays ANOVA results in a clear, structured table emphasizing sums of squares and variance explained.
-   `TukeyHSD()` *(stats)*
    -   Performs post-hoc pairwise comparisons following a significant ANOVA.
-   `plot()` *(base R)*
    -   Visualizes post-hoc comparison results, such as Tukeyâ€™s HSD confidence intervals.
-   `aictab()` *(AICcmodavg)*
    -   Compares competing statistical models using Akaike Information Criterion (AIC).

## Example APA-style Write-up {#sec-anova-apa}

The following example demonstrates one acceptable way to report the results of a one-way analysis of variance (ANOVA) in APA style.

**One-Way Analysis of Variance (ANOVA)**

A one-way analysis of variance (ANOVA) revealed a significant effect of study method on memory test scores (see @tbl-anova-supernova), $F(2, 57) = 24.72, p < .001$. Post hoc Tukeyâ€™s HSD tests indicated that participants in the testing condition scored significantly higher than participants in the flashcards and rereading conditions. Mean scores (*M* Â± *SD*) were 75.2 Â± 8.1 for flashcards, 69.8 Â± 9.0 for rereading, and 84.9 Â± 7.1 for testing, demonstrating that the testing method produced the highest memory performance.

## ðŸ’¡ *Reproducibility Tip:* {#sec-anova-tip}

When your analysis involves randomness-such as simulated data, resampling, or random assignmentâ€”always set a seed using `set.seed()`.

Because we used `set.seed(123)` when generating the data in this chapter, anyone who runs the code will obtain identical results. This ensures that differences in output are due to changes in code or assumptions, not randomness.

Setting a seed is a simple but essential step for reproducible research, especially when teaching, sharing code, or revisiting an analysis later.

It does not matter what the seed is set to, as long as it is set to something!
